# Survey of Vision-and-Language Navigation

This is the official repository of "**[Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models](https://arxiv.org/pdf/2407.07035)**", a comprehensive survey
of recent progress in VLN with foundation models.

## ğŸ‘  Our Survey has been officially accepted by TMLR!!!

## Introduction

Vision-and-Language Navigation (VLN) has gained increasing attention over recent years and many approaches have emerged to advance their development. The remarkable achievements of foundation models have shaped the challenges and proposed methods for VLN research. In this survey, we provide a top-down review that adopts a principled framework for embodied planning and reasoning and emphasizes the current methods and future opportunities leveraging foundation models to address VLN challenges. We hope our in-depth discussions could provide valuable resources and insights: on the one hand, to document the progress and explore opportunities and potential roles for foundation models in this field, and on the other, to organize different challenges and solutions in VLN to foundation model researchers.

## Citation

If you find our work useful in your research, please consider citing:

    @article{zhang2024vision,
      title={Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models},
      author={Zhang, Yue and Ma, Ziqiao and Li, Jialu and Qiao, Yanyuan and Wang, Zun and Chai, Joyce and Wu, Qi and Bansal, Mohit and Kordjamshidi, Parisa},
      journal={arXiv preprint arXiv:2407.07035},
      year={2024}
    }

ğŸ”” We will update this page frequently. If you believe additional work should be included, please do not hesitate to email us (zhan1624@msu.edu) or raise an issue. Your suggestions and comments are invaluable to ensuring the completeness of our resources.

## Content

---

- [Relevant Surveys](#relevant-surveys)
- [World Model](#word-modal)
- [Human Model](#human-modal)
- [VLN Agent](#vln-agent-learning-an-embodied-agent-for-reasoning-and-planning)
- [Behavior Analysis of the VLN Agent](#behavir-analysis)

---

## Relevant Surveys

| Title                                                                                                                           | Venue | Date |                                   Code                                   |
| :------------------------------------------------------------------------------------------------------------------------------ | :---: | :--: | :----------------------------------------------------------------------: |
| [**Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions**](https://arxiv.org/abs/2203.12667)    |  ACL  | 2022 | [Github](https://github.com/eric-ai-lab/awesome-vision-language-navigation) |
| [**Visual language navigation: A survey and open challenges**](https://link.springer.com/article/10.1007/s10462-022-10174-9) |   -   | 2023 |                                    -                                    |
| [**Vision-Language Navigation: A Survey and Taxonomy**](https://arxiv.org/abs/2108.11544)                                    |   -   | 2021 |                                    -                                    |

---

## World Model

A world model helps the VLN agent to understand their surrounding environments, predict how their actions would change the world state, and align their perception and actions with language instructions.

| Title                                                                                                                                                                                                                                          |  Venue  | Date |                                  Code                                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :--: | :--------------------------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [**MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation**](https://arxiv.org/pdf/2502.13451)                                                                                      |   ACL   | 2025 |            [Github](https://github.com/linglingxiansen/MapNav)            | åˆ©ç”¨æ ‡æ³¨è¯­ä¹‰åœ°å›¾ï¼ˆASMï¼Œè¯¥åœ°å›¾æä¾›ç‰©ç†éšœç¢ç‰©ã€å·²æ¢ç´¢åŒºåŸŸã€æ™ºèƒ½ä½“å½“å‰ä½ç½®ã€è¿åŠ¨è½¨è¿¹åŠè¯­ä¹‰å¯¹è±¡ç­‰ä¿¡æ¯ï¼‰æ›¿ä»£å†å²å¸§ã€‚å½“å‰RGBè§‚æµ‹ç”»é¢ã€ASMå’Œå¯¼èˆªæŒ‡ä»¤ä½œä¸ºç«¯åˆ°ç«¯è§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶çš„è¾“å…¥ï¼Œè¯¥æ¡†æ¶ä¼šç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°çš„å¯¼èˆªåŠ¨ä½œã€‚                                                                                                                                                                                                                                           |
| [**VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation**](https://arxiv.org/abs/2402.03561)                                                                                                                      |  AAAI  | 2024 |                                   -                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Volumetric Environment Representation for Vision-Language Navigation**](https://arxiv.org/pdf/2403.14158)                                                                                                                                |  CVPR  | 2024 |              [Github](https://github.com/DefaultRui/VLN-VER)              |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Vision Language Navigation with Knowledge-driven Environmental Dreamer**](https://www.ijcai.org/proceedings/2023/0204.pdf)                                                                                                               |  IJCAI  | 2023 |                                   -                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Frequency-enhanced Data Augmentation for Vision-and-Language Navigation**](https://openreview.net/pdf?id=eKFrXWb0sT)                                                                                                                     | NeurIPS | 2023 |         [Github](https://github.com/hekj/FDA?tab=readme-ov-file)         |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation**](https://proceedings.neurips.cc/paper_files/paper/2023/file/0d9e08f247ca7fbbfd5e50b7ff9cf357-Paper-Conference.pdf)                                             | NeurIPS | 2023 |                   [Github](https://github.com/hekj/FDA)                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation**](https://arxiv.org/abs/2305.19195)                                                                                                       | NeurIPS | 2023 |             [Github](https://github.com/jialuli-luka/PanoGen)             | å›¾ç”Ÿæ–‡â†’æ–‡ç”Ÿå›¾ï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰â†’ç”Ÿæˆå…¨æ™¯å›¾ï¼ˆæ—‹è½¬ç›¸æœºè§†è§’ã€é€’å½’å¤–ç»˜ï¼‰ã€‚å¾—åˆ°æ–°çš„ç¯å¢ƒæ•°æ®é›†ã€‚                                                                                                                                                                                                                                                                                                                                                                        |
| [**Simple and Effective Synthesis of Indoor 3D Scenes**](https://arxiv.org/pdf/2204.02960)                                                                                                                                                  |  AAAI  | 2023 |            [Github](https://github.com/google-research/se3ds)            | ä»å•å¼ æˆ–å°‘é‡å›¾åƒåˆæˆæ²‰æµ¸å¼3Då®¤å†…åœºæ™¯ã€‚ç›®æ ‡æ˜¯ä»æ–°è§†è§’ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒå’Œè§†é¢‘ï¼ŒåŒ…æ‹¬é‚£äº›è¿œè¶…å‡ºè¾“å…¥å›¾åƒè§†è§’èŒƒå›´ä½†ä»ä¿æŒ3Dä¸€è‡´æ€§çš„è§†ç‚¹ã€‚                                                                                                                                                                                                                                                                                                                            |
| [**Learning Navigational Visual Representations with Semantic Map Supervision**](https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf) |  ICCV  | 2023 |                                   -                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Learning vision-and-language navigation from youtube videos**](https://arxiv.org/abs/2307.11984)                                                                                                                                         |  ICCV  | 2023 |           [Github](https://github.com/JeremyLinky/YouTube-VLN)           | ä»ç½‘ç»œè§†é¢‘ä¸­å­¦ä¹ å¯¼èˆªç­–ç•¥ï¼Œå®ç°è¿™ä¸€ç›®æ ‡éœ€è¦è§£å†³ä¸¤å¤§æŒ‘æˆ˜ï¼šè‡ªåŠ¨æ„å»ºè·¯å¾„-æŒ‡ä»¤å¯¹ï¼Œä»¥åŠä»åŸå§‹æ— æ ‡æ³¨è§†é¢‘ä¸­æŒ–æ˜çœŸå®ç©ºé—´å¸ƒå±€çŸ¥è¯†ã€‚Youtube è§†é¢‘é¢„è®­ç»ƒï¼Œä½¿ç”¨ä¸‹æ¸¸æ•°æ®é›†ï¼ˆR2Rï¼‰å¾®è°ƒã€‚                                                                                                                                                                                                                                                                                      |
| [**GridMM: Grid Memory Map for Vision-and-Language Navigation**](https://arxiv.org/abs/2307.12907)                                                                                                                                          |  ICCV  | 2023 |                [Github](https://github.com/MrZihan/GridMM)                | ä¸ºè¡¨å¾å…ˆå‰è®¿é—®è¿‡çš„ç¯å¢ƒï¼Œç°æœ‰VLNæ–¹æ³•å¤§å¤šé‡‡ç”¨å¾ªç¯çŠ¶æ€ã€æ‹“æ‰‘åœ°å›¾æˆ–è‡ªä¸Šè€Œä¸‹è¯­ä¹‰åœ°å›¾ä½œä¸ºè®°å¿†æ¨¡å—ã€‚ä¸æ­¤ä¸åŒï¼Œæˆ‘ä»¬æ„å»ºäº†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒã€åŠ¨æ€ç”Ÿé•¿çš„ç½‘æ ¼è®°å¿†åœ°å›¾ï¼ˆGridMMï¼‰æ¥ç»“æ„åŒ–å·²æ¢ç´¢ç¯å¢ƒã€‚                                                                                                                                                                                                                                                                           |
| [**BEVBert: Multimodal Map Pre-training for Language-guided Navigation**](https://arxiv.org/abs/2212.04385)                                                                                                                                 |  ICCV  | 2023 |             [Github](https://github.com/MarSaKi/VLN-BEVBert)             | å¤§è§„æ¨¡é¢„è®­ç»ƒåœ¨è§†è§‰ä¸è¯­è¨€å¯¼èˆª(VLN)ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—æˆæ•ˆã€‚ç„¶è€Œç°æœ‰é¢„è®­ç»ƒæ–¹æ³•å¤§å¤šé‡‡ç”¨ç¦»æ•£å…¨æ™¯å›¾æ¥å­¦ä¹ è§†è§‰-æ–‡æœ¬å…³è”ï¼Œè¿™è¦æ±‚æ¨¡å‹éšå¼å…³è”å…¨æ™¯å›¾ä¸­ä¸å®Œæ•´ä¸”é‡å¤çš„è§‚æµ‹ç‰‡æ®µï¼Œå¯èƒ½æŸå®³æ™ºèƒ½ä½“çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹åŸºäºåœ°å›¾çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ç©ºé—´æ„ŸçŸ¥æœºåˆ¶æå‡VLNæ€§èƒ½ã€‚                                                                                                                                                                                     |
| [**Scaling Data Generation in Vision-and-Language Navigation**](https://arxiv.org/abs/2307.15644)                                                                                                                                           |  ICCV  | 2023 | [Github](https://github.com/wz0919/ScaleVLN/tree/main?tab=readme-ov-file) | ä¸ºç¼“è§£ç°æœ‰è§†è§‰è¯­è¨€å¯¼èˆªæ•°æ®é›†æ™®éå­˜åœ¨çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤§è§„æ¨¡å­¦ä¹ æ•°æ®ç”ŸæˆèŒƒå¼â€”â€”åŸºäºHM3Då’ŒGibsonæ•°æ®é›†çš„1200ä½™ä¸ªé€¼çœŸè™šæ‹Ÿç¯å¢ƒï¼Œåˆ©ç”¨ç½‘ç»œå¯è·å–èµ„æºåˆæˆäº†490ä¸‡æ¡æŒ‡ä»¤-è½¨è¿¹å¯¹ã€‚ç ”ç©¶é‡ç‚¹æ­ç¤ºäº†è¯¥èŒƒå¼ä¸­å„ç»„ä»¶å¯¹æ™ºèƒ½ä½“æ€§èƒ½çš„å½±å“æœºåˆ¶ï¼Œå¹¶æ¢ç´¢äº†å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¢å¼ºæ•°æ®å®ç°æ™ºèƒ½ä½“çš„é¢„è®­ç»ƒä¸å¾®è°ƒã€‚                                                                                                                                                        |
| [**A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning**](https://arxiv.org/abs/2210.03112)                                                                                               |  CVPR  | 2023 |                [Github](https://github.com/clin1223/MTVM)                | æœ¬ç ”ç©¶æå‡ºé€šè¿‡åˆæˆæŒ‡ä»¤è¿›è¡Œå¤§è§„æ¨¡æ•°æ®å¢å¼ºçš„åˆ›æ–°æ–¹æ¡ˆï¼šåŸºäº500ä½™ä¸ªå®¤å†…åœºæ™¯çš„å¯†é›†é‡‡æ ·360åº¦å…¨æ™¯å›¾æ„å»ºå¯¼èˆªè½¨è¿¹ï¼Œå¹¶é‡‡ç”¨é«˜è´¨é‡å¤šè¯­è¨€å¯¼èˆªæŒ‡ä»¤ç”Ÿæˆå™¨Marky[63]ä¸ºæ¯æ¡è½¨è¿¹ç”Ÿæˆè§†è§‰ grounded çš„å¯¼èˆªæŒ‡ä»¤ï¼›åŒæ—¶åˆ©ç”¨å›¾åƒç”Ÿæˆå¯¹æŠ—ç½‘ç»œ[27]åˆæˆæ–°è§†è§’ä¸‹çš„è§‚æµ‹å›¾åƒã€‚æœ€ç»ˆæ„å»ºçš„420ä¸‡æ¡æŒ‡ä»¤-è½¨è¿¹é…å¯¹æ•°æ®é›†ï¼Œå…¶è§„æ¨¡è¶…è¶Šç°æœ‰äººå·¥æ ‡æ³¨æ•°æ®é›†ä¸¤ä¸ªæ•°é‡çº§ï¼Œä¸”æ¶µç›–æ›´ä¸°å¯Œçš„ç¯å¢ƒç±»å‹ä¸è§‚å¯Ÿè§†è§’ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬å°†æŒ‡ä»¤ã€çŠ¶æ€ä¸åŠ¨ä½œå†å²ã€å½“å‰è§‚å¯ŸåŠå€™é€‰åŠ¨ä½œè¾“å…¥å¤šæ¨¡æ€è½¬æ¢å™¨ï¼Œä»¥é¢„æµ‹ä¸‹ä¸€åŠ¨ä½œã€‚ |
| [**EnvEdit: Environment Editing for Vision-and-Language Navigation**](https://arxiv.org/abs/2203.15685)                                                                                                                                     |  CVPR  | 2022 |             [Github](https://github.com/jialuli-luka/VLN-SIG)             | æå‡ºENVEDITæ•°æ®å¢å¼ºæ–¹æ³•â€”â€”é€šè¿‡å¯¹ç°æœ‰ç¯å¢ƒè¿›è¡Œç¼–è¾‘æ¥åˆ›å»ºæ–°ç¯å¢ƒï¼Œä»è€Œè®­ç»ƒå‡ºæ›´å…·æ³›åŒ–èƒ½åŠ›çš„æ™ºèƒ½ä½“ã€‚ç»ç¼–è¾‘å¢å¼ºçš„ç¯å¢ƒå¯åœ¨é£æ ¼ã€ç‰©ä½“å¤–è§‚å’Œç‰©ä½“ç±»åˆ«ä¸‰ä¸ªç»´åº¦ä¸Šä¸åŸå§‹ç¯å¢ƒå½¢æˆå·®å¼‚ã€‚åœ¨è¿™äº›ç¼–è¾‘å¢å¼ºç¯å¢ƒä¸Šçš„è®­ç»ƒèƒ½æœ‰æ•ˆé˜²æ­¢æ™ºèƒ½ä½“å¯¹ç°æœ‰ç¯å¢ƒçš„è¿‡æ‹Ÿåˆï¼Œå¹¶æå‡å…¶åœ¨æ–°ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚                                                                                                                                                                                             |
| [**Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation**](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960375.pdf)                                                                         |  ECCV  | 2022 |             [Github](https://github.com/jialuli-luka/VLN-SIG)             | è¿‘æœŸåŸºäºTransformerçš„VLNæ–¹æ³•é€šè¿‡å¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ç›´æ¥è¿æ¥è§†è§‰è§‚å¯Ÿä¸è¯­è¨€æŒ‡ä»¤ï¼Œå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œè¿™äº›æ–¹æ³•é€šå¸¸é‡‡ç”¨LSTMè§£ç å™¨å°†æ—¶åºä¸Šä¸‹æ–‡è¡¨ç¤ºä¸ºå›ºå®šé•¿åº¦å‘é‡ï¼Œæˆ–é€šè¿‡äººå·¥è®¾è®¡çš„éšè—çŠ¶æ€æ„å»ºå¾ªç¯Transformerã€‚è€ƒè™‘åˆ°å•ä¸€å›ºå®šé•¿åº¦å‘é‡å¾€å¾€éš¾ä»¥æ•æ‰é•¿æœŸæ—¶åºä¸Šä¸‹æ–‡ï¼Œæœ¬æ–‡æå‡ºå…·æœ‰å¯å˜é•¿åº¦è®°å¿†çš„å¤šæ¨¡æ€Transformerï¼ˆMTVMï¼‰ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡æ—¶åºä¸Šä¸‹æ–‡æ¥å®ç°è§†è§‰åŸºç¡€çš„è‡ªç„¶è¯­è¨€å¯¼èˆªã€‚                                                                                        |
| [**How Much Can CLIP Benefit Vision-and-Language Tasks?**](https://arxiv.org/abs/2107.06383)                                                                                                                                                |  ICLR  | 2022 |              [Github](https://github.com/clip-vil/CLIP-ViL)              |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation**](https://arxiv.org/abs/2202.11742)                                                                                                            |  CVPR  | 2022 |               [Github](https://github.com/cshizhe/VLN-DUET)               |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**History Aware Multimodal Transformer for Vision-and-Language Navigation**](https://arxiv.org/abs/2110.13309)                                                                                                                             | NeurIPS | 2021 |        [Github](https://cshizhe.github.io/projects/vln_hamt.html)        |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Pathdreamer: A World Model for Indoor Navigation**](https://arxiv.org/abs/2105.08756)                                                                                                                                                    |  ICCV  | 2021 |                                   -                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Episodic Transformer for Vision-and-Language Navigation**](https://openaccess.thecvf.com/content/ICCV2021/papers/Pashevich_Episodic_Transformer_for_Vision-and-Language_Navigation_ICCV_2021_paper.pdf)                                  |  ICCV  | 2021 |                                   -                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Airbert: In-domain Pretraining for Vision-and-Language Navigation**](https://arxiv.org/abs/2108.09105)                                                                                                                                   |  ICCV  | 2021 |                 [Github](https://airbert-vln.github.io/)                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [**Vision-Language Navigation with Random Environmental Mixup**](https://arxiv.org/abs/2106.07876)                                                                                                                                          |  ICCV  | 2021 |               [Github](https://github.com/LCFractal/VLNREM)               |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |

## Human Model: Interpreting and Communication with Humans

The human model comprehends human-provided natural language instructions per situation to complete navigation tasks.

| Title                                                                                                                                                                                                                             | Venue | Date |                                    Code                                    |                                    Description                                    |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--: | :-------------------------------------------------------------------------: | --------------------------------------------------------------------------- |
| [**Scene Map-based Prompt Tuning for Navigation Instruction Generation**](https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_Scene_Map-based_Prompt_Tuning_for_Navigation_Instruction_Generation_CVPR_2025_paper.pdf)   | CVPR | 2025 |                                      -                                      |                                                                            |
| [**NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM**](https://arxiv.org/pdf/2502.11142)                                                                                    |  ACL  | 2025 |                  [Github](https://github.com/MrZihan/NavRAG)                  |                  ä¸»è¦ä»‹ç»äº†ä¸€ç§åä¸º NavRAG çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆç”¨æˆ·éœ€æ±‚å¯¼å‘çš„å¯¼èˆªæŒ‡ä»¤ï¼Œä»¥è§£å†³å…·èº«å¯¼èˆªï¼ˆVision-and-Language Navigation, VLNï¼‰é¢†åŸŸä¸­é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚NavRAG åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰æ¡†æ¶å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé€šè¿‡æ„å»ºåˆ†å±‚åœºæ™¯æè¿°æ ‘æ¥ç†è§£3Dç¯å¢ƒï¼Œå¹¶ç»“åˆç”¨æˆ·è§’è‰²æ¨¡æ‹Ÿç”Ÿæˆå¤šæ ·åŒ–çš„å¯¼èˆªæŒ‡ä»¤ï¼Œå…¶ç”Ÿæˆçš„å¤šæ ·åŒ–æŒ‡ä»¤æœ‰åŠ©äºæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç”¨æˆ·éœ€æ±‚å’Œåœºæ™¯ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨æŒ‡ä»¤å¤šæ ·æ€§å’Œç”¨æˆ·éœ€æ±‚åŒ¹é…ä¸Šçš„ä¸è¶³ã€‚                  |
| [**Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel**](https://arxiv.org/abs/2412.08467)                                                                                                     | ICLR | 2025 |                  [Github](https://github.com/wz0919/VLN-SRDF)                  |                  æå‡ºäº†ä¸€ç§åˆ›æ–°çš„è‡ªç²¾åŒ–æ•°æ®é£è½®æ–¹æ³•ï¼Œé€šè¿‡å¯¼èˆªå™¨å’ŒæŒ‡ä»¤ç”Ÿæˆå™¨çš„ååŒä¼˜åŒ–ï¼Œæ„å»ºäº†é«˜è´¨é‡çš„VLNæ•°æ®é›†ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚                  |
| [**Navigation Instruction Generation with BEV Perception and Large Language Models**](https://arxiv.org/pdf/2407.15087)                                                                                                        | ECCV | 2024 |               [Github](https://github.com/FanScy/BEVInstructor)               |               ä¸»è¦ä»‹ç»äº†ä¸€ç§åä¸ºBEVInstructorçš„æ–°æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¯¼èˆªæŒ‡ä»¤ã€‚è¯¥æ–¹æ³•ç»“åˆäº†é¸Ÿç°å›¾ï¼ˆBEVï¼‰ç‰¹å¾å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œä»¥æé«˜3Dæ„ŸçŸ¥å’Œè¯­è¨€èƒ½åŠ›ã€‚               |
| [**Controllable Navigation Instruction Generation with Chain of Thought Prompting**](https://arxiv.org/pdf/2407.07433)                                                                                                         | ECCV | 2024 |                [Github](https://github.com/refkxh/C-Instructor)                |                ä¸»è¦ä»‹ç»äº†ä¸€ç§åä¸ºC-Instructorçš„å¯æ§å¯¼èˆªæŒ‡ä»¤ç”Ÿæˆæ¨¡å‹ï¼Œå…¶æ ¸å¿ƒç›®æ ‡æ˜¯ç”Ÿæˆé«˜è´¨é‡ã€é£æ ¼å¯æ§ä¸”å†…å®¹å¯æ§çš„å¯¼èˆªæŒ‡ä»¤ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­è¨€èƒ½åŠ›ä¸å¤šç§æŠ€æœ¯åˆ›æ–°ï¼ŒåŒ…æ‹¬é“¾å¼æ€ç»´åœ°æ ‡æœºåˆ¶ï¼ˆCoTLï¼‰ã€ç©ºé—´æ‹“æ‰‘å»ºæ¨¡ä»»åŠ¡ï¼ˆSTMTï¼‰å’Œé£æ ¼æ··åˆè®­ç»ƒï¼ˆSMTï¼‰ï¼Œä»è€Œæ˜¾è‘—æå‡äº†ç”ŸæˆæŒ‡ä»¤çš„å¯æ‰§è¡Œæ€§å’Œå¯æ§æ€§                |
| [**Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation**](https://aclanthology.org/2024.acl-long.734.pdf)                                                                                        |  ACL  | 2024 |                [Github](https://github.com/gmuraleekrishna/SAS)                |                                |
| [**Correctable Landmark Discovery via Large Models for Vision-Language Navigation**](https://arxiv.org/abs/2405.18721)                                                                                                         | TPAMI | 2024 |                [Github](https://github.com/expectorlin/CONSOLE)                |                                |
| [**NavHint: Vision and Language Navigation Agent with a Hint Generator**](https://arxiv.org/pdf/2402.02559)                                                                                                                    | EACL | 2024 |                    [Github](https://github.com/HLR/NavHint)                    |                                        |
| [**Learning to Follow and Generate Instructions for Language-Capable Navigation**](https://ieeexplore.ieee.org/document/10359152)                                                                                              | TPAMI | 2023 |                                      -                                      |                                                                            |
| [**A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning**](https://arxiv.org/pdf/2210.03112)                                                                                  | CVPR | 2023 | [Dataset](https://github.com/google-research-datasets/RxR/tree/main/marky-mT5) |  |
| [**Lana: A Language-Capable Navigator for Instruction Following and Generation**](https://arxiv.org/abs/2303.08409)                                                                                                            | CVPR | 2023 |                 [Github](https://github.com/wxh1996/LANA-VLN)                 |                                  |
| [**KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation**](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf) | CVPR | 2023 |                [Github](https://github.com/xiangyangli-cn/KERM)                |                                |
| [**PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation**](https://arxiv.org/pdf/2305.11918)                                                                                           |  MM  | 2023 |                                      -                                      |                                                                            |
| [**CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation**](https://arxiv.org/abs/2103.00852)                                                            |   -   | 2023 |                                      -                                      |                                                                            |
| [**VLN-Trans: Translator for the Vision and Language Navigation Agent**](https://arxiv.org/pdf/2302.09230)                                                                                                                     |  ACL  | 2023 |                   [Github](https://github.com/HLR/VLN-trans)                   |                                      |
| [**Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration**](https://arxiv.org/pdf/2203.04006)                                                                                                 |  ACL  | 2022 |               [Github](https://github.com/liangcici/Probes-VLN)               |                              |
| [**Less is More: Generating Grounded Navigation Instructions from Landmarks**](https://arxiv.org/pdf/2004.14973)                                                                                                               | CVPR | 2022 | [Github](https://github.com/google-research-datasets/RxR/tree/main/marky-mT5) |  |
| [**On the Evaluation of Vision-and-Language Navigation Instructions**](https://arxiv.org/abs/2101.10504)                                                                                                                       | EACL | 2021 |                                      -                                      |                                                                            |
| [**Do As I Can, Not As I Say:Grounding Language in Robotic Affordances**](https://say-can.github.io/assets/palm_saycan.pdf)                                                                                                    |   -   |  -  |                      [Github](https://say-can.github.io/)                      |                                            |

## VLN Agent: Learning an Embodied Agent for Reasoning and Planning

| Title                                                                                                                                                                                                                                                                   |     Venue     | Date |                                             Code                                             |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------: | :--: | :-------------------------------------------------------------------------------------------: |
| [**SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts**](https://arxiv.org/pdf/2412.05552)                                                                                                                              |      ICCV      | 2025 |                           [Github](https://github.com/GengzeZhou/SAME)                           |
| [**MiniVLN: Efficient Vision-and-Language Navigation byProgressive Knowledge Distillation**](https://arxiv.org/pdf/2409.18800)                                                                                                                                       |      ICRA      | 2024 |                                               -                                               |
| [**Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation**](https://arxiv.org/pdf/2302.06072)                                                                                                                                                 |      AAAI      | 2023 |                                               -                                               |
| [**Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation**](https://arxiv.org/abs/2308.12587)                                                                                                                                            |      ICCV      | 2023 |                          [Github](https://github.com/CSir1996/VLN-GELA)                          |
| [**Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation**](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf)                                 |      ICCV      | 2023 |                           [Github](https://github.com/chengaopro/AZHP)                           |
| [**Bird's-Eye-View Scene Graph for Vision-Language Navigation**](https://arxiv.org/abs/2308.04758)                                                                                                                                                                   |      ICCV      | 2023 |                                               -                                               |
| [**Masked Path Modeling for Vision-and-Language Navigation**](https://arxiv.org/abs/2305.14268)                                                                                                                                                                      | EMNLP Findings | 2023 |                                               -                                               |
| [**Improving Vision-and-Language Navigation by Generating Future-View Image Semantics**](https://arxiv.org/pdf/2304.04907)                                                                                                                                           |      CVPR      | 2023 |                        [Github](https://github.com/jialuli-luka/VLN-SIG)                        |
| [**HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation**](https://ieeexplore.ieee.org/document/10006384)                                                                                                                          |     TPAMI     | 2023 |                                               -                                               |
| [**Target-Driven Structured Transformer Planner for Vision-Language Navigation**](https://arxiv.org/pdf/2207.11201)                                                                                                                                                  |       MM       | 2022 |                         [Github](https://github.com/YushengZhao/TD-STP)                         |
| [**HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation**](https://ieeexplore.ieee.org/document/9880046)                                                                                                                                     |      CVPR      | 2022 |                         [Github](https://github.com/YanyuanQiao/HOP-VLN)                         |
| [**LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation**](https://aclanthology.org/2022.coling-1.505.pdf)                                                                                                                              |     COLING     | 2022 |                              [Github](https://github.com/HLR/LOViS)                              |
| [**Scene-Intuitive Agent for Remote Embodied Visual Grounding**](https://arxiv.org/pdf/2103.12944)                                                                                                                                                                   |      CVPR      | 2021 |                                               -                                               |
| [**SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation**](https://arxiv.org/abs/2110.14143)                                                                                                                                               |    NeurIPS    | 2021 |                                               -                                               |
| [**The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation**](https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf) |      ICCV      | 2021 | [Github]([https://github.com/YicongHong/Recurrent-VLN-BERT](https://github.com/YuankaiQi/ORIST)) |
| [**VLN BERT: A Recurrent Vision-and-Language BERT for Navigation**](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_VLN_BERT_A_Recurrent_Vision-and-Language_BERT_for_Navigation_CVPR_2021_paper.pdf)                                                     |      CVPR      | 2021 |                    [Github](https://github.com/YicongHong/Recurrent-VLN-BERT)                    |
| [**Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training**](https://arxiv.org/abs/2002.10638)                                                                                                                                         |      CVPR      | 2020 |                        [Github](https://github.com/weituo12321/PREVALENT)                        |

### VLN-CE Agent

| Title                                                                                                                                                   | Venue | Date |                               Code                               |
| :------------------------------------------------------------------------------------------------------------------------------------------------------ | :---: | :--: | :--------------------------------------------------------------: |
| [**MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming**](https://arxiv.org/pdf/2508.02549)                                      | ICCV | 2025 |                                -                                |
| [**JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation**](https://arxiv.org/pdf/2509.22548)       | Arxiv | 2025 |      [Github](https://miv-xjtu.github.io/JanusVLN.github.io/)      |
| [**NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments**](https://arxiv.org/pdf/2506.23468)          | ICCV | 2025 |          [Github](https://github.com/Feliciaxyao/NavMorph)          |
| [**Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation**](https://arxiv.org/abs/2407.05890)              | AAAI | 2025 |                                -                                |
| [**Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation**](https://arxiv.org/pdf/2404.01943)          | CVPR | 2024 |            [Github](https://github.com/MrZihan/HNR-VLN)            |
| [**ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments**](https://arxiv.org/abs/2304.03047v2)            | PAMI | 2024 |   [Github](https://github.com/MarSaKi/ETPNav?tab=readme-ov-file)   |
| [**Narrowing the Gap between Vision and Action in Navigation**](https://www.arxiv.org/abs/2408.10388)                                                |  MM  | 2024 |                                -                                |
| [**BEVBert: Multimodal Map Pre-training for Language-guided Navigation**](https://arxiv.org/pdf/2212.04385)                                          | ICCV | 2023 | [Github](https://github.com/MarSaKi/VLN-BEVBert?tab=readme-ov-file) |
| [**Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation**](https://arxiv.org/abs/2203.02764) | CVPR | 2022 |   [Github](https://github.com/YicongHong/Discrete-Continuous-VLN)   |
| [**Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments**](https://arxiv.org/abs/2004.02857)                              | ECCV | 2020 |           [Github](https://github.com/jacobkrantz/VLN-CE)           |

### LLM/VLM-based VLN Agent

#### Zero-shot

| Title                                                                                                                                                                                                                                                |      Venue      | Date |                     Code                     |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------: | :--: | :-------------------------------------------: |
| [**LLM as Copilot for Coarse-grained Vision-and-Language Navigation**](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf)                                                                                                        |       ECCV       | 2024 |                       -                       |
| [**Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions**](https://ieeexplore.ieee.org/abstract/document/10611565)                                                                                                      |       ICRA       | 2024 | [Github](https://github.com/LYX0501/DiscussNav) |
| [**MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation**](https://arxiv.org/abs/2401.07314)                                                                                                               |       ACL       | 2024 |  [Github](https://chen-judge.github.io/MapGPT/)  |
| [**MC-GPT: Empowering Vision-and-LanguageNavigation with Memory Map and Reasoning Chains**](https://arxiv.org/pdf/2405.10620)                                                                                                                     |        -        | 2024 |                       -                       |
| [**InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment**](https://arxiv.org/pdf/2406.04882)                                                                                                                |        -        | 2024 | [Github](https://github.com/LYX0501/InstructNav) |
| [**NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models**](https://arxiv.org/abs/2305.16986)                                                                                                                   |       AAAI       | 2024 |  [Github](https://github.com/GengzeZhou/NavGPT)  |
| [**March in Chat: Interactive Prompting for Remote Embodied Referring Expression**](https://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf) |       ICCV       | 2023 |   [Github](https://github.com/YanyuanQiao/MiC)   |
| [**Vision and Language Navigation in the Real World via Online Visual Language Mapping**](https://arxiv.org/pdf/2310.10822)                                                                                                                       |        -        | 2023 |                       -                       |
| [**A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models**](https://peihaochen.github.io/files/publications/A2Nav.pdf)                                                                    | NeurIPS Workshop | 2023 |                       -                       |
| [**CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation**](https://arxiv.org/pdf/2211.16649)                                                                                                                                         |        -        | 2022 |                       -                       |

#### Fine-tuning

| Title                                                                                                                                    |     Venue     | Date |                      Code                      |
| :--------------------------------------------------------------------------------------------------------------------------------------- | :------------: | :--: | :---------------------------------------------: |
| [**EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation**](https://arxiv.org/pdf/2506.01551)         |     Arxiv     | 2025 | [Github](https://github.com/expectorlin/EvolveNav) |
| [**LangNav: Language as a Perceptual Representation for Navigation**](https://aclanthology.org/2024.findings-naacl.60.pdf)            | NACCL Findings | 2024 |  [Github](https://github.com/pbw-Berwin/LangNav)  |
| [**NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning**](https://arxiv.org/abs/2403.07376) |       -       | 2024 |  [Github](https://github.com/expectorlin/NavCoT)  |
| [**Towards Learning a Generalist Model for Embodied Navigation**](https://arxiv.org/abs/2312.02010)                                   |      CVPR      | 2024 |   [Github](https://github.com/LaVi-Lab/NaviLLM)   |
| [**NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models**](https://www.arxiv.org/abs/2407.12366)   |      ECCV      | 2024 |  [Github](https://github.com/GengzeZhou/NavGPT-2)  |
| [**NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation**](https://arxiv.org/pdf/2402.15852)                 |      RSS      | 2024 |  [Github](https://github.com/GengzeZhou/NavGPT-2)  |

## Behavior Analysis of the VLN Agent

| Title                                                                                                                               |     Venue     | Date |                      Code                      |
| :---------------------------------------------------------------------------------------------------------------------------------- | :------------: | :--: | :---------------------------------------------: |
| [**Do Visual Imaginations Improve Vision-and-Language Navigation Agents?**](https://arxiv.org/pdf/2503.16394)                    |      CVPR      | 2025 |                        -                        |
| [**Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation**](https://arxiv.org/pdf/2409.17313)          | EMNLP Findings | 2024 | [Github](https://github.com/zehao-wang/navnuances) |
| [**Behavioral Analysis of Vision-and-Language Navigation Agents**](https://yoark.github.io/assets/pdf/vln-behave/vln-behave.pdf) |      CVPR      | 2023 |   [Github](https://github.com/Yoark/vln-behave)   |
| [**Diagnosing Vision-and-Language Navigation: What Really Matters**](https://aclanthology.org/2022.naacl-main.438.pdf)           |     NACCL     | 2022 |   [Github](https://github.com/VegB/Diagnose_VLN)   |
